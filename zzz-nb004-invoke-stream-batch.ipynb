{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "465c003a-29cf-4cfe-a0c6-d36c04ae2b37",
   "metadata": {},
   "source": [
    "# Alternative ways to execute runnables\n",
    "* Invoke, Stream and Batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5c33d0-1805-4810-95ac-394fa19ce0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fecd39d0-e72e-4bc2-8a68-2fa4008ea365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f4a923-b19e-498e-9be5-e47ec4a77d80",
   "metadata": {},
   "source": [
    "#### Install LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a04d63-0f4a-411d-94f9-decf8ed7af0e",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1cf94ae-6c39-4475-9c5b-4b74d8d78753",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189e9e17-dfb0-4fd3-85b9-1fba83771941",
   "metadata": {},
   "source": [
    "## Connect with an LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46a004d-4c98-47ab-b720-0f09c94d42b5",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "148df8e0-361d-4ddd-8709-af48fa1648d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1998155-91de-4cbc-bc88-8d77beefb51b",
   "metadata": {},
   "source": [
    "* NOTE: Since right now is the best LLM in the market, we will use OpenAI by default. You will see how to connect with other Open Source LLMs like Llama3 or Mistral in a next lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ae8595e-5c07-4b02-8a79-db55fd357527",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190b5d23-5d09-4c88-b32f-3299795cd91b",
   "metadata": {},
   "source": [
    "## LCEL Chain\n",
    "* **An LCEL Chain is a Sequence of Runnables.**\n",
    "* Almost any component in LangChain (prompts, models, output parsers, vector store retrievers, tools, etc) can be used as a Runnable.\n",
    "* Runnables can be chained together using the pipe operator `|`. The resulting chains of runnables are also runnables themselves.\n",
    "* The order (left to right) of the elements in a LCEL chain matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5a34484-0981-4edc-8ba2-7c8521d28c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "246e32f8-4380-41d2-b33a-04ff4085569b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Despite his incredible success as a footballer, Ronaldo has never scored a goal from a free kick in a major international tournament such as the World Cup or the European Championship.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"tell me a curious fact about {soccer_player}\")\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "chain.invoke({\"soccer_player\": \"Ronaldo\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1026d5-2e99-4fd4-af2a-ac2c6d2eeeac",
   "metadata": {},
   "source": [
    "* All the components of the chain are Runnables.\n",
    "* **When we write chain.invoke() we are using invoke with all the componentes of the chain in an orderly manner**:\n",
    "    * First, we apply .invoke() with the user input to the prompt template.\n",
    "    * Then, with the completed prompt, we apply .invoke() to the model.\n",
    "    * And finally, we apply .invoke() to the output parser with the output of the model.\n",
    "* IMPORTANT: the order of operations in a chain matters. If you try to execute the previous chain with the components in different order, the chain will fail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c825056a-404d-4630-b5be-77130efaf0f8",
   "metadata": {},
   "source": [
    "## Let's see this graphically"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31565f6-f04f-4afb-91c6-598814fd0877",
   "metadata": {},
   "source": [
    "![alt text](./lcel-2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dafb72-bea0-422b-93ba-d8a0ab244019",
   "metadata": {},
   "source": [
    "## And now let's replicate this process without using the LCEL chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a6c802-2e5f-46e7-970b-5e336489c03e",
   "metadata": {},
   "source": [
    "#### First, we apply .invoke() with the user input to the prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c5fbd44-6969-4d8f-ba83-ddd9e7d72193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content='tell me a curious fact about Ronaldo')])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.invoke({\"soccer_player\": \"Ronaldo\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9ab4a4-999d-4640-9a62-886e4fdc5ce2",
   "metadata": {},
   "source": [
    "#### Then, with the completed prompt, we apply .invoke() to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ede59f87-0be9-4fda-b6c4-dd521b62f2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages.human import HumanMessage\n",
    "\n",
    "output_after_first_step = [HumanMessage(content='tell me a curious fact about Ronaldo')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7606951-561a-4679-8f59-8fdbd097734c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='One curious fact about Cristiano Ronaldo is that he is known for his incredible work ethic and dedication to training. He is known to be one of the hardest working players in the world, often staying late after training to work on his skills and fitness. Ronaldo is also known for his strict diet and fitness routine, which has helped him maintain his high level of performance throughout his career.', response_metadata={'token_usage': {'completion_tokens': 75, 'prompt_tokens': 14, 'total_tokens': 89, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-a5292070-6234-4abf-8899-c9b4f35c3b80-0', usage_metadata={'input_tokens': 14, 'output_tokens': 75, 'total_tokens': 89})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(output_after_first_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19cc252-e57b-49e3-9887-7f4ccacbb4de",
   "metadata": {},
   "source": [
    "#### And finally, we apply .invoke() to the output parser with the output of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a157968-1b54-4cf3-bc1d-d4ee4ff2b3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages.ai import AIMessage\n",
    "\n",
    "output_after_second_step = AIMessage(content='One curious fact about Cristiano Ronaldo is that he does not have any tattoos on his body. Despite the fact that many professional athletes have tattoos, Ronaldo has chosen to keep his body ink-free.', response_metadata={'token_usage': {'completion_tokens': 38, 'prompt_tokens': 14, 'total_tokens': 52}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-c9812511-043a-458a-bfb8-005bc0d057fb-0', usage_metadata={'input_tokens': 14, 'output_tokens': 38, 'total_tokens': 52})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "074ea557-69b2-4eed-b9ba-4344d15c35d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One curious fact about Cristiano Ronaldo is that he does not have any tattoos on his body. Despite the fact that many professional athletes have tattoos, Ronaldo has chosen to keep his body ink-free.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser.invoke(output_after_second_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123eadeb-a912-4c8b-b42e-af1a5f848977",
   "metadata": {},
   "source": [
    "## Ways to execute Runnables\n",
    "* Remember:\n",
    "    * An LCEL Chain is a Sequence of Runnables.\n",
    "    * Almost any component in LangChain (prompts, models, output parsers, etc) can be used as a Runnable.\n",
    "    * Runnables can be chained together using the pipe operator `|`. The resulting chains of runnables are also runnables themselves.\n",
    "    * The order (left to right) of the elements in a LCEL chain matters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf49277-35e5-4061-94f0-c79270203846",
   "metadata": {},
   "source": [
    "#### LCEL Chains/Runnables are used with:\n",
    "* chain.invoke(): call the chain on an input.\n",
    "* chain.stream(): call the chain on an input and stream back chunks of the response.\n",
    "* chain.batch(): call the chain on a list of inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3fe651b-dc5b-4067-a430-a4fa5da3faa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me one sentence about {politician}.\")\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd006047-876e-4d5a-a116-e4124c4324af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Winston Churchill was a British statesman who served as Prime Minister during World War II.', response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 14, 'total_tokens': 32, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-c4e449c8-54a3-4525-abe5-72f2479f6c2b-0', usage_metadata={'input_tokens': 14, 'output_tokens': 18, 'total_tokens': 32})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"politician\": \"Churchill\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89db19ed-7425-4506-956f-4ebb3eae44f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F.D. Roosevelt was the only president to serve more than two terms in office."
     ]
    }
   ],
   "source": [
    "for s in chain.stream({\"politician\": \"F.D. Roosevelt\"}):\n",
    "    print(s.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea804f6-d7f1-41aa-a6ef-ee668b8d0705",
   "metadata": {},
   "source": [
    "#### The for loop explained in simple terms\n",
    "This `for` loop is used to show responses piece by piece as they are received. Here's how it works in simple terms:\n",
    "\n",
    "1. **Getting Responses in Parts**: The loop receives pieces of a response from the system one at a time. In this example, the system is responding with information about the politician \"F.D. Roosevelt.\"\n",
    "\n",
    "2. **Printing Out Responses Immediately**: Each time a new piece of the response is received, it immediately prints it out. The setup makes sure there are no new lines between parts, so it all looks like one continuous response.\n",
    "\n",
    "3. **No Waiting**: By using this loop, you don't have to wait for the entire response to be ready before you start seeing parts of it. This makes it feel quicker and more like a conversation.\n",
    "\n",
    "This way, the loop helps provide a smoother and more interactive way of displaying responses from the system as they are generated.\n",
    "\n",
    "#### The for loop explained in technical terms\n",
    "This `for` loop is used to handle streaming output from a language model response. Hereâ€™s a breakdown of its functionality and context:\n",
    "\n",
    "1. **Iteration through Streamed Output**: The `for` loop iterates over the generator returned by `chain.stream(...)`. The `stream` method of the `chain` object (which is a combination of a prompt template and a language model) is designed to yield responses incrementally. This is particularly useful when responses from a model are long or need to be displayed in real-time.\n",
    "\n",
    "2. **Data Fetching**: In this loop, `s` represents each piece of content that is streamed back from the model as the response is being generated. The model in this case is set up to respond with information about the politician \"F.D. Roosevelt\".\n",
    "\n",
    "3. **Output Handling**: Inside the loop, `print(s.content, end=\"\", flush=True)` is called for each piece of streamed content. The `print` function is customized with:\n",
    "   - `end=\"\"`: This parameter ensures that each piece of content is printed without adding a new line after each one, thus allowing the response to appear continuous on a single line.\n",
    "   - `flush=True`: This parameter forces the output buffer to be flushed immediately after each print statement, ensuring that each piece of content is displayed to the user as soon as it is received without any delay.\n",
    "\n",
    "By using this loop, the code is able to display each segment of the model's response as it becomes available, making the user interface more dynamic and responsive, especially for real-time applications where prompt feedback is beneficial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b680921b-e75f-4d77-b655-22dc9a91e9d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='Lenin was a Russian revolutionary and politician who played a key role in the establishment of the Soviet Union.', response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 14, 'total_tokens': 35, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-d1d71d9e-8d6e-4bb7-88c6-7b11144dd759-0', usage_metadata={'input_tokens': 14, 'output_tokens': 21, 'total_tokens': 35}),\n",
       " AIMessage(content='Stalin was a ruthless dictator who ruled the Soviet Union with an iron fist.', response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 14, 'total_tokens': 30, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-6d040847-3458-4aae-9adb-b3bb0f48f89b-0', usage_metadata={'input_tokens': 14, 'output_tokens': 16, 'total_tokens': 30})]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.batch([{\"politician\": \"Lenin\"}, {\"politician\": \"Stalin\"}])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c0a455-5f6b-4b32-8cef-4d6ccce13526",
   "metadata": {},
   "source": [
    "#### LCEL Chains/Runnables can be also used asynchronously:\n",
    "* chain.ainvoke(): call the chain on an input.\n",
    "* chain.astream(): call the chain on an input and stream back chunks of the response.\n",
    "* chain.abatch(): call the chain on a list of inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5e97ca-f124-4c0d-9565-2f60a1b92d49",
   "metadata": {},
   "source": [
    "## How to execute the code from Visual Studio Code\n",
    "* In Visual Studio Code, see the file 004-invoke-stream-batch.py\n",
    "* In terminal, make sure you are in the directory of the file and run:\n",
    "    * python 004-invoke-stream-batch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3feaa8ab-caa4-48a5-9a12-c14881889bad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmapp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
